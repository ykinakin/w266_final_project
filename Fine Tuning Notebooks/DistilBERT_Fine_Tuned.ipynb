{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de1076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import create_optimizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "from transformers import TFAutoModelForMaskedLM\n",
    "from transformers.keras_callbacks import PushToHubCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f216583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c553da",
   "metadata": {},
   "source": [
    "Initially, load the DistilBERT base model and tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a801c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eeb1d1",
   "metadata": {},
   "source": [
    "Check to see if GPUs are available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bfbca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392d048",
   "metadata": {},
   "source": [
    "Open the pickled list of data scraped from geovernment reports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf7843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"../convert.pkl\", \"rb\")\n",
    "text = pickle.load(text_file)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05327819",
   "metadata": {},
   "source": [
    "These processing of data loading model training is all based upon the tutorial provided on https://huggingface.co/course/chapter7/3?fw=tf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e185584",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0984ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text= []\n",
    "for item in text:\n",
    "    item.replace('\\n', '')\n",
    "    item.replace('\\t', '')\n",
    "    if sum(c.isdigit() for c in item)/len(item) > 0.1 or len(item) == 0: \n",
    "        clean_text.append(item)\n",
    "\n",
    "text_df = pd.DataFrame({'text':clean_text})\n",
    "dataset = Dataset.from_pandas(text_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645964b",
   "metadata": {},
   "source": [
    "Check the number of reports that are predominantly clean text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ff176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8776e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90154 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f867cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd50749",
   "metadata": {},
   "source": [
    "Check the number of resultant chunks that meet the criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd04ca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51634"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eefceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(0.9*len(lm_datasets))\n",
    "test_size = len(lm_datasets) - train_size\n",
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59928618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b4c1be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = len(tf_train_dataset)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "#model_name = model_checkpoint.split(\"/\")[-1]\n",
    "#callback = PushToHubCallback(\n",
    "#    output_dir=f\"{model_name}-finetuned-imdb\", tokenizer=tokenizer\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f4daf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1452/1452 [==============================] - 805s 551ms/step - loss: 2.0379 - val_loss: 1.6205\n",
      "Epoch 2/3\n",
      "1452/1452 [==============================] - 800s 551ms/step - loss: 1.7003 - val_loss: 1.6203\n",
      "Epoch 3/3\n",
      "1452/1452 [==============================] - 799s 551ms/step - loss: 1.7030 - val_loss: 1.6181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa09408f2b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, batch_size=16, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b43123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"The mineral [MASK] is found in the rock [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0bbbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e427e92c9f9e449b82a24cd4f3d1d470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.03453386202454567,\n",
       "   'token': 26427,\n",
       "   'token_str': 'carbonate',\n",
       "   'sequence': '[CLS] the mineral carbonate is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.032245464622974396,\n",
       "   'token': 4221,\n",
       "   'token_str': '##ite',\n",
       "   'sequence': '[CLS] the mineralite is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.02590017393231392,\n",
       "   'token': 19057,\n",
       "   'token_str': 'chloride',\n",
       "   'sequence': '[CLS] the mineral chloride is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.023440266028046608,\n",
       "   'token': 15772,\n",
       "   'token_str': 'oxide',\n",
       "   'sequence': '[CLS] the mineral oxide is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.02240598201751709,\n",
       "   'token': 3514,\n",
       "   'token_str': 'oil',\n",
       "   'sequence': '[CLS] the mineral oil is found in the rock [MASK]. [SEP]'}],\n",
       " [{'score': 0.18109388649463654,\n",
       "   'token': 13197,\n",
       "   'token_str': 'formations',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock formations. [SEP]'},\n",
       "  {'score': 0.12895970046520233,\n",
       "   'token': 22913,\n",
       "   'token_str': 'strata',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock strata. [SEP]'},\n",
       "  {'score': 0.11249405890703201,\n",
       "   'token': 9014,\n",
       "   'token_str': 'layers',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock layers. [SEP]'},\n",
       "  {'score': 0.0432974249124527,\n",
       "   'token': 4195,\n",
       "   'token_str': 'formation',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock formation. [SEP]'},\n",
       "  {'score': 0.026585068553686142,\n",
       "   'token': 9607,\n",
       "   'token_str': 'veins',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock veins. [SEP]'}]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "unmasker(\"The mineral [MASK] is found in the rock [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7001d715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.10689971596002579,\n",
       "   'token': 3989,\n",
       "   'token_str': '##ization',\n",
       "   'sequence': '[CLS] the mineralization is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.06298457831144333,\n",
       "   'token': 4221,\n",
       "   'token_str': '##ite',\n",
       "   'sequence': '[CLS] the mineralite is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.05126945674419403,\n",
       "   'token': 15707,\n",
       "   'token_str': '##ogy',\n",
       "   'sequence': '[CLS] the mineralogy is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.04947109520435333,\n",
       "   'token': 4180,\n",
       "   'token_str': 'content',\n",
       "   'sequence': '[CLS] the mineral content is found in the rock [MASK]. [SEP]'},\n",
       "  {'score': 0.027343131601810455,\n",
       "   'token': 3430,\n",
       "   'token_str': 'material',\n",
       "   'sequence': '[CLS] the mineral material is found in the rock [MASK]. [SEP]'}],\n",
       " [{'score': 0.07103662192821503,\n",
       "   'token': 4195,\n",
       "   'token_str': 'formation',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock formation. [SEP]'},\n",
       "  {'score': 0.06592454761266708,\n",
       "   'token': 13197,\n",
       "   'token_str': 'formations',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock formations. [SEP]'},\n",
       "  {'score': 0.060979198664426804,\n",
       "   'token': 9014,\n",
       "   'token_str': 'layers',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock layers. [SEP]'},\n",
       "  {'score': 0.05580151453614235,\n",
       "   'token': 8168,\n",
       "   'token_str': 'samples',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock samples. [SEP]'},\n",
       "  {'score': 0.03865545988082886,\n",
       "   'token': 9607,\n",
       "   'token_str': 'veins',\n",
       "   'sequence': '[CLS] the mineral [MASK] is found in the rock veins. [SEP]'}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model=model, tokenizer = tokenizer)\n",
    "unmasker(\"The mineral [MASK] is found in the rock [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21673143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"GeoDistilBERT.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
